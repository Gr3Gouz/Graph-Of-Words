{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I) IMPORTING TRAINING AND TESTING SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Notebook_Graph_of_Words.ipynb\\n', 'README.md\\n', 'r8-test-stemmed.txt\\n', 'r8-train-stemmed.txt\\n']\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from pandas import read_csv\n",
    "import os\n",
    "import time\n",
    "import networkx as nx\n",
    "import time\n",
    "print os.popen(\"ls\").readlines()\n",
    "\n",
    "train_set = read_csv('r8-train-stemmed.txt', header = None, sep='\\t', names = ['label', 'document'])\n",
    "X_train = train_set['document']\n",
    "Y_train = train_set['label']\n",
    "\n",
    "test_set = read_csv('r8-test-stemmed.txt', header = None, sep='\\t', names = ['label', 'document'])\n",
    "X_test = test_set['document']\n",
    "Y_test = test_set['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) BUILDING THE BAG OF WORDS FOR TRAINING AND TESTING SETS #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Building the BoW for the training set *****\n",
      "***** Building the BoW for the testing set *****\n",
      "Done in 0.76 seconds\n",
      "(5485, 14575) <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_bag_of_words(X):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=1)\n",
    "    X_train_bag_of_words = vectorizer.fit_transform(X)\n",
    "    return (vectorizer, X_train_bag_of_words)\n",
    "\n",
    "tic=time.time()\n",
    "print '***** Building the BoW for the training set *****'\n",
    "(vectorizer_train, X_train_bag_of_words) = get_bag_of_words(X_train)\n",
    "print '***** Building the BoW for the testing set *****'\n",
    "X_test_bag_of_words = vectorizer_train.transform(X_test)\n",
    "toc=time.time()-tic\n",
    "print 'Done in {0} seconds'.format(round(toc,2))\n",
    "\n",
    "print X_train_bag_of_words.shape, type(X_train_bag_of_words) #nb lines = nb documents / nb columns = nb features (i.e nb tokens, i.e nb words)\n",
    "#X_tfidf is an object \"scipy.sparse.csr.csr_matrix\" (to store sparse matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III) BUILDING THE GRAPHS OF WORDS FOR TRAINING AND TESTING SETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) 1. Defining all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isdirected=False\n",
    "isweighted=True\n",
    "size_window=4\n",
    "\n",
    "centrality_measures=['pagerank','degree']\n",
    "centrality_measure=centrality_measures[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) 2. Functions to build the graphs of words for every document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_graph_from_document(doc_str, isdirected, isweighted, size_window):\n",
    "    #doc_str is a string (i.e the document)\n",
    "    #doc_str should have more words than size_window\n",
    "    import networkx as nx\n",
    "    doc_array = doc_str.split()\n",
    "    N = len(doc_array)\n",
    "    \n",
    "    if isdirected:\n",
    "        G = nx.DiGraph()\n",
    "    else:\n",
    "        G=nx.Graph()\n",
    "        \n",
    "    for j in range(N):\n",
    "        for i in range(max(j-size_window+1,0),j):\n",
    "            if G.has_edge(doc_array[i], doc_array[j]):\n",
    "                if isweighted:\n",
    "                    # we added this one before, just increase the weight by one\n",
    "                    G[doc_array[i]][doc_array[j]]['weight'] += 1\n",
    "            else:\n",
    "                # new edge. add with weight=1\n",
    "                G.add_edge(doc_array[i], doc_array[j], weight=1)\n",
    "\n",
    "    return G\n",
    "\n",
    "def get_gow(corpus, isdirected, isweighted, size_window):\n",
    "    dict_graph_of_words = dict()\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "            dict_graph_of_words[i] = get_graph_from_document(corpus[i],isdirected,isweighted, size_window)\n",
    "        \n",
    "    return dict_graph_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) 3. Building the graph of words for every training document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Building the graph of words for every training document *****\n",
      "Directed Graphs : False\n",
      "Weighted Graphs : True\n",
      "Window size : 4\n",
      "Done in 4.97 seconds\n"
     ]
    }
   ],
   "source": [
    "t_begin = time.time()    \n",
    "print '***** Building the graph of words for every training document *****'\n",
    "print 'Directed Graphs : '+ str(isdirected)     \n",
    "print 'Weighted Graphs : '+ str(isweighted)\n",
    "print 'Window size : ' + str(size_window)\n",
    "dict_graph_of_words = get_gow(X_train,isdirected,isweighted,size_window)\n",
    "t_end = time.time() - t_begin\n",
    "print 'Done in {0} seconds'.format(round(t_end,2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) 4. Functions to compute the graph-based IDF values from the training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_graph_from_all_documents(corpus, isdirected, isweighted, size_window):\n",
    "    # corpus is X_train\n",
    "    import networkx as nx\n",
    "    \n",
    "    if isdirected:\n",
    "        G = nx.DiGraph()\n",
    "    else:\n",
    "        G=nx.Graph()\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        doc_array=corpus[i].split()\n",
    "        N = len(doc_array)\n",
    "        \n",
    "        for j in range(N):\n",
    "            for i in range(max(j-size_window+1,0),j):\n",
    "                if G.has_edge(doc_array[i], doc_array[j]):\n",
    "                    if isweighted:\n",
    "                        # we added this one before, just increase the weight by one\n",
    "                        G[doc_array[i]][doc_array[j]]['weight'] += 1\n",
    "                else:\n",
    "                    # new edge. add with weight=1\n",
    "                    G.add_edge(doc_array[i], doc_array[j], weight=1)\n",
    "\n",
    "    return G\n",
    "\n",
    "def idf_graph_words_corpus(graph_all_docs, vocab, centrality_measure):\n",
    "    # vocab : dict('word':index_column)\n",
    "    dict_idf_graph=dict()\n",
    "    \n",
    "    if centrality_measure=='pagerank':\n",
    "        iterator_measure=nx.pagerank(graph_all_docs, max_iter=150).iteritems()\n",
    "    elif centrality_measure=='degree':\n",
    "        iterator_measure=nx.betweenness_centrality(graph_all_docs,weight='weight').iteritems()\n",
    "    for word, centrality_value in iterator_measure:\n",
    "        dict_idf_graph[vocab[word]] = idf_graph(centrality_value,graph_all_docs)\n",
    "    \n",
    "    return dict_idf_graph # returns a dictionary with centrality values of aggregated graph (or a function of them)\n",
    "\n",
    "def idf_graph(centrality_value,graph_all_docs):\n",
    "    import numpy as np\n",
    "    return np.log(graph_all_docs.number_of_nodes()/float(centrality_value)) \n",
    "    # computes a formula for idf, given centrality value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) 5. Create a graph for the whole corpus of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Building a graph of words for the whole training corpus *****\n",
      "Directed Graphs : False\n",
      "Weighted Graphs : True\n",
      "Window size : 4\n",
      "Number of nodes in the graph : 14575\n",
      "Done in 3.42 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "print '***** Building a graph of words for the whole training corpus *****'\n",
    "print 'Directed Graphs : '+ str(isdirected)     \n",
    "print 'Weighted Graphs : '+ str(isweighted)\n",
    "print 'Window size : ' + str(size_window)\n",
    "graph_all_docs=get_graph_from_all_documents(X_train,isdirected,isweighted,size_window)\n",
    "toc = time.time() - tic\n",
    "print 'Number of nodes in the graph : '+ str(graph_all_docs.number_of_nodes()) # we find the same number of words in the dictionnary\n",
    "print 'Done in {0} seconds'.format(round(toc,2))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) 6. Find the graph-based IDF values for the graph of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Computing the graph-based IDF values for the training corpus *****\n",
      "Directed Graphs : False\n",
      "Weighted Graphs : True\n",
      "Window size : 4\n",
      "Done in 28.94 seconds\n"
     ]
    }
   ],
   "source": [
    "vocab_train = vectorizer_train.vocabulary_\n",
    "t_begin = time.time()  \n",
    "print '***** Computing the graph-based IDF values for the training corpus *****'\n",
    "print 'Directed Graphs : '+ str(isdirected)     \n",
    "print 'Weighted Graphs : '+ str(isweighted)\n",
    "print 'Window size : ' + str(size_window)\n",
    "dict_idf_graph=idf_graph_words_corpus(graph_all_docs,vocab_train,centrality_measure)\n",
    "t_end = time.time() - t_begin\n",
    "print 'Done in {0} seconds'.format(round(t_end,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) 7. Define a function that builds the graph-based TW-IDF matrix for a given corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gow_to_sparse_matrix(dict_graph_of_words, shape_bag_of_words, vocab,dict_idf_graph, centrality_measure):\n",
    "    #vocab : dict('word':index_column)\n",
    "    from scipy.sparse import dok_matrix\n",
    "    \n",
    "    X_graph_of_words = dok_matrix(shape_bag_of_words)\n",
    "    \n",
    "    for i in range(len(dict_graph_of_words)):\n",
    "        if (i%1000 == 0):\n",
    "            print '{0} out of {1}'.format(i, len(dict_graph_of_words))\n",
    "        if centrality_measure=='pagerank':\n",
    "            iterator_measure=nx.pagerank(dict_graph_of_words[i],max_iter=150).iteritems()\n",
    "        elif centrality_measure=='degree':\n",
    "            iterator_measure=nx.betweenness_centrality(dict_graph_of_words[i],weight='weight').iteritems()\n",
    " \n",
    "        for word, centrality_value in iterator_measure:\n",
    "            if word in vocab:\n",
    "                X_graph_of_words[i,vocab[word]] = centrality_value*dict_idf_graph[vocab[word]]\n",
    "                # We can even compute a different function of these two terms (with logarithmic weighting)\n",
    "                \n",
    "    return X_graph_of_words.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) 8. Compute the graph-based TW-IDF matrix for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Computing the graph-TW-IDF matrix for the training corpus *****\n",
      "Directed Graphs : False\n",
      "Weighted Graphs : True\n",
      "Window size : 4\n",
      "Centrality measure : pagerank\n",
      "0 out of 5485\n",
      "1000 out of 5485\n",
      "2000 out of 5485\n",
      "3000 out of 5485\n",
      "4000 out of 5485\n",
      "5000 out of 5485\n",
      "Done in 63.61 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "print '***** Computing the graph-TW-IDF matrix for the training corpus *****'\n",
    "print 'Directed Graphs : '+ str(isdirected)     \n",
    "print 'Weighted Graphs : '+ str(isweighted)\n",
    "print 'Window size : ' + str(size_window)\n",
    "print 'Centrality measure : ' + str(centrality_measure)\n",
    "X_train_graph_of_words = gow_to_sparse_matrix(dict_graph_of_words, X_train_bag_of_words.shape, vocab_train, dict_idf_graph,centrality_measure)\n",
    "toc = time.time() - tic\n",
    "print 'Done in {0} seconds'.format(round(toc,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) 9. Compute the graph-based TW-IDF matrix for the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Computing the graph-TW-IDF matrix for the testing corpus *****\n",
      "Directed Graphs : False\n",
      "Weighted Graphs : True\n",
      "Window size : 4\n",
      "Centrality measure : pagerank\n",
      "0 out of 2189\n",
      "1000 out of 2189\n",
      "2000 out of 2189\n",
      "Done in 26.54 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "print '***** Computing the graph-TW-IDF matrix for the testing corpus *****'\n",
    "print 'Directed Graphs : '+ str(isdirected)     \n",
    "print 'Weighted Graphs : '+ str(isweighted)\n",
    "print 'Window size : ' + str(size_window)\n",
    "print 'Centrality measure : ' + str(centrality_measure)\n",
    "dict_test_graph_of_words = get_gow(X_test,isdirected,isweighted,size_window)\n",
    "vocab_train = vectorizer_train.vocabulary_\n",
    "X_test_graph_of_words = gow_to_sparse_matrix(dict_test_graph_of_words, (len(X_test), len(vocab_train)), vocab_train, dict_idf_graph, centrality_measure)\n",
    "toc = time.time() - tic\n",
    "print 'Done in {0} seconds'.format(round(toc,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV) CLASSIFICATION PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_feature_selection = False # If True : Recursive Feature Elimination\n",
    "nb_features_to_select = 300\n",
    "with_SVD=False # Use SVD reduced values, or not (change to False, only with linearSVM learning)\n",
    "graph_features=False # Change to True if you want to see the cv-plots for feature selection without SVD\n",
    "\n",
    "classifiers_text=['linearSVM','gaussianSVM','logistic_reg','Adaboost']\n",
    "string_classifier=classifiers_text[0]\n",
    "\n",
    "if string_classifier=='gaussianSVM':\n",
    "    with_SVD=True\n",
    "if use_feature_selection:\n",
    "    with_SVD=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V) DIMENSIONALITY REDUCTION : SVD Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Computing SVD Decomposition for BoW and GoW with 500 components *****\n",
      "Done in 10.9 seconds\n",
      "Explained variance ratio for BoW: 58.6360111278 %\n",
      "Explained variance ratio for GoW : 80.5184797473 %\n",
      "***** SVD-Transform of X_test *****\n",
      "Done in 0.18 seconds\n"
     ]
    }
   ],
   "source": [
    "if with_SVD :\n",
    "    from sklearn.decomposition import TruncatedSVD #absolutely need to upgrade scikitlearn to 0.15.2\n",
    "    n_components=500\n",
    "    \n",
    "    print '***** Computing SVD Decomposition for BoW and GoW with {0} components *****'.format(n_components)\n",
    "    tic = time.time()\n",
    "    svd_bag = TruncatedSVD(n_components = n_components, n_iter = 5, random_state=42)\n",
    "    svd_graph = TruncatedSVD(n_components = n_components, n_iter = 5, random_state=42)\n",
    "    svd_bag.fit(X_train_bag_of_words)\n",
    "    svd_graph.fit(X_train_graph_of_words)\n",
    "    toc = time.time() - tic\n",
    "    print 'Done in {0} seconds'.format(round(toc,2))\n",
    "    \n",
    "    print 'Explained variance ratio for BoW: ' + str(100*svd_bag.explained_variance_ratio_.sum())+ ' %'\n",
    "    print 'Explained variance ratio for GoW : ' + str(100*svd_graph.explained_variance_ratio_.sum())+ ' %'\n",
    "    \n",
    "    X_bag_svd_reduced = svd_bag.transform(X_train_bag_of_words)\n",
    "    X_graph_svd_reduced = svd_graph.transform(X_train_graph_of_words)\n",
    "    \n",
    "    # Reduce test set with SVD decomposition\n",
    "    print '***** SVD-Transform of X_test *****'\n",
    "    tic = time.time()\n",
    "    X_test_bag_svd_reduced = svd_bag.transform(X_test_bag_of_words)\n",
    "    X_test_graph_svd_reduced = svd_graph.transform(X_test_graph_of_words)\n",
    "    print 'Done in {0} seconds'.format(round(time.time() - tic,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI) CLASSIFICATION TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Classification task *****\n"
     ]
    }
   ],
   "source": [
    "print '***** Classification task *****'\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import KFold\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "Y = Y_train\n",
    "cv=KFold(X_train.shape[0], n_folds=5,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI) 1. RBF SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross-validation for RBF SVM and BoW *****\n",
      "Gamma : 1/1, Cost : 1/1\n",
      "Maximum f1 score obtained for Cost 0.1, Gamma 0.1 : 12.437740681 %\n",
      "Done in 340.555930138 seconds\n",
      "***** Cross-validation for RBF SVM and GoW *****\n",
      "Gamma : 0.1, Cost : 0.1\n",
      "Maximum f1 score obtained for Cost 0.1, Gamma 0.1 : 78.5717272712 %\n",
      "Done in 280.953822851 seconds\n"
     ]
    }
   ],
   "source": [
    "if (string_classifier=='gaussianSVM'): \n",
    "\n",
    "    # BAG OF WORDS\n",
    "    X = X_bag_svd_reduced\n",
    "    score_cv = dict()\n",
    "    gamma_list = [0.1] #, 1.0, 10.0] takes too much time otherwise\n",
    "    C_list = [0.1] #, 1.0, 10.0]\n",
    "    print '***** Cross-validation for RBF SVM and BoW *****'\n",
    "    tic = time.time()\n",
    "    for gamma_cv in gamma_list:\n",
    "        for C_cv in C_list:\n",
    "            print 'Gamma : {0}/{1}, Cost : {2}/{3}'.format(gamma_list.index(gamma_cv)+1,len(gamma_list),C_list.index(C_cv)+1,len(C_list))\n",
    "            clf = svm.SVC(kernel='rbf', C=C_cv, gamma = gamma_cv,class_weight='auto')\n",
    "            score_cv[(C_cv, gamma_cv)] = np.mean(cross_validation.cross_val_score(clf, X, Y, cv=cv, scoring='f1_weighted'))\n",
    "            \n",
    "    (C_opt, gamma_opt) = max(score_cv.iteritems(), key=operator.itemgetter(1))[0]\n",
    "    print 'Maximum f1 score obtained for Cost ' + str(C_opt)+','+ ' Gamma '+str(gamma_opt)+' : '+ str(100*max(score_cv.values()))+ ' %'\n",
    "    \n",
    "    clf_bag_rbf = svm.SVC(kernel='rbf', C=C_opt, gamma = gamma_opt,class_weight='auto')\n",
    "    clf_bag_rbf.fit(X,Y)\n",
    "    print 'Done in {0} seconds'.format(time.time()-tic)\n",
    "    \n",
    "    \n",
    "    # GRAPH OF WORDS\n",
    "    X = X_graph_svd_reduced\n",
    "    score_cv = dict()\n",
    "    gamma_list = [0.1] #, 1.0, 10.0]\n",
    "    C_list = [0.1] #, 1.0, 10.0]\n",
    "    print '***** Cross-validation for RBF SVM and GoW *****'\n",
    "    tic = time.time()\n",
    "    for gamma_cv in gamma_list:\n",
    "        for C_cv in C_list:\n",
    "            print 'Gamma : {0}, Cost : {1}'.format(gamma_cv,C_cv)\n",
    "            clf = svm.SVC(kernel='rbf', C=C_cv, gamma = gamma_cv,class_weight='auto') # Watch out the class_weight\n",
    "            score_cv[(C_cv, gamma_cv)] = np.mean(cross_validation.cross_val_score(clf, X, Y, cv=cv, scoring='f1_weighted'))\n",
    "            \n",
    "    (C_opt, gamma_opt) = max(score_cv.iteritems(), key=operator.itemgetter(1))[0]\n",
    "    print 'Maximum f1 score obtained for Cost ' + str(C_opt)+','+ ' Gamma '+str(gamma_opt)+' : '+ str(100*max(score_cv.values()))+ ' %'\n",
    "    \n",
    "    clf_graph_rbf = svm.SVC(kernel='rbf', C=C_opt, gamma = gamma_opt,class_weight='auto')\n",
    "    clf_graph_rbf.fit(X,Y)\n",
    "    print 'Done in {0} seconds'.format(time.time()-tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI) 2. Linear SVM without Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross-validation for linear SVM and BoW *****\n",
      "Cost : 1/3\n",
      "Cost : 2/3\n",
      "Cost : 3/3\n",
      "Maximum f1 score obtained for Cost 10.0 : 97.1114829082 %\n",
      "Done in 3.71687793732 seconds\n",
      "***** Cross-validation for linear SVM and GoW *****\n",
      "Cost : 1/3\n",
      "Cost : 2/3\n",
      "Cost : 3/3\n",
      "Maximum f1 score obtained for Cost 10.0 : 97.2384260877 %\n",
      "Done in 6.9106631279 seconds\n"
     ]
    }
   ],
   "source": [
    "if (string_classifier=='linearSVM')&(not use_feature_selection):\n",
    "\n",
    "    # Cross-validate for cost parameter C\n",
    "    score_cv = dict()\n",
    "    C_list = [0.1,1.0,10.0]\n",
    "    \n",
    "    # Bag of Words\n",
    "    X=X_train_bag_of_words\n",
    "    if with_SVD:\n",
    "        X=X_bag_svd_reduced\n",
    "        \n",
    "    print '***** Cross-validation for linear SVM and BoW *****'\n",
    "    tic=time.time()\n",
    "    for C_cv in C_list:\n",
    "            print 'Cost : {0}/{1}'.format(C_list.index(C_cv)+1,len(C_list))\n",
    "            clf=svm.LinearSVC(C=C_cv,class_weight='auto')\n",
    "            score_cv[(C_cv)] = np.mean(cross_validation.cross_val_score(clf, X, Y, cv=cv, scoring='f1_weighted'))\n",
    "    \n",
    "    C_opt = max(score_cv.iteritems(), key=operator.itemgetter(1))[0]\n",
    "    print 'Maximum f1 score obtained for Cost ' + str(C_opt)+' : '+ str(100*max(score_cv.values()))+ ' %'\n",
    "    \n",
    "    clf_bag=svm.LinearSVC(C=C_opt,class_weight='auto')\n",
    "    clf_bag.fit(X,Y)\n",
    "    print 'Done in {0} seconds'.format(time.time()-tic)\n",
    "    \n",
    "    # Graph of words\n",
    "    X=X_train_graph_of_words\n",
    "    if with_SVD:\n",
    "        X=X_graph_svd_reduced\n",
    "    print '***** Cross-validation for linear SVM and GoW *****'\n",
    "    tic=time.time()\n",
    "    for C_cv in C_list:\n",
    "            print 'Cost : {0}/{1}'.format(C_list.index(C_cv)+1,len(C_list))\n",
    "            clf=svm.LinearSVC(C=C_cv,class_weight='auto') # class_weight auto gives relative importance to classes\n",
    "            score_cv[(C_cv)] = np.mean(cross_validation.cross_val_score(clf, X, Y, cv=cv, scoring='f1_weighted'))\n",
    "    \n",
    "    C_opt = max(score_cv.iteritems(), key=operator.itemgetter(1))[0]\n",
    "    print 'Maximum f1 score obtained for Cost ' + str(C_opt)+' : '+ str(100*max(score_cv.values()))+ ' %'\n",
    "    \n",
    "    clf_graph=svm.LinearSVC(C=C_opt,class_weight='auto')\n",
    "    clf_graph.fit(X,Y)\n",
    "    print 'Done in {0} seconds'.format(time.time()-tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI) 3. Linear SVM with Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross-validation for linearSVM and BoW, without SVD, with feature selection *****\n",
      "300/14575 features selected\n",
      "Done in 13.81 seconds\n",
      "\n",
      "Cost : 1/3\n",
      "Cost : 2/3\n",
      "Cost : 3/3\n",
      "Maximum f1 score obtained for Cost 10.0 : 97.6301889293 %\n",
      "Done in 1.42059397697 seconds\n",
      "***** Cross-validation for linearSVM and GoW, without SVD, with feature selection *****\n",
      "300/14575 features selected\n",
      "Done in 33.14 seconds\n",
      "Cost : 1/3\n",
      "Cost : 2/3\n",
      "Cost : 3/3\n",
      "Maximum f1 score obtained for Cost 10.0 : 97.6482536078 %\n",
      "Done in 1.35689687729 seconds\n"
     ]
    }
   ],
   "source": [
    "# Cross-validate for cost parameter C\n",
    "score_cv = dict()\n",
    "C_list = [0.1,1.0,10.0]\n",
    "\n",
    "if (string_classifier=='linearSVM')&use_feature_selection:\n",
    "    \n",
    "    ###### Bag of words on feature-cleaned data #######\n",
    "    from sklearn.feature_selection import RFE\n",
    "    print '***** Cross-validation for linearSVM and BoW, without SVD, with feature selection *****'\n",
    "    \n",
    "    clf=svm.LinearSVC(C=1.0,class_weight='auto')\n",
    "    tic = time.time()\n",
    "    nb_features_to_remove_at_each_step = 200 # We don't want it to be too slow\n",
    "    rfecv_bag = RFE(clf, nb_features_to_select, step=nb_features_to_remove_at_each_step)\n",
    "    rfecv_bag.fit(X_train_bag_of_words, Y)\n",
    "    toc = round(time.time() - tic,2)\n",
    "    print '{0}/{1} features selected'.format(rfecv_bag.n_features_,X_train_bag_of_words.shape[1])\n",
    "    print 'Done in {0} seconds'.format(toc)\n",
    "    X = rfecv_bag.transform(X_train_bag_of_words)\n",
    "    X_test_bag_feature_selected = rfecv_bag.transform(X_test_bag_of_words)\n",
    "    \n",
    "    print\n",
    "    tic=time.time()\n",
    "    for C_cv in C_list:\n",
    "            print 'Cost : {0}/{1}'.format(C_list.index(C_cv)+1,len(C_list))\n",
    "            clf=svm.LinearSVC(C=C_cv,class_weight='auto')\n",
    "            score_cv[(C_cv)] = np.mean(cross_validation.cross_val_score(clf, X, Y, cv=cv, scoring='f1_weighted'))\n",
    "            \n",
    "    C_opt = max(score_cv.iteritems(), key=operator.itemgetter(1))[0]\n",
    "    print 'Maximum f1 score obtained for Cost ' + str(C_opt)+' : '+ str(100*max(score_cv.values()))+ ' %'\n",
    "    \n",
    "    clf_bag_non_reduced=svm.LinearSVC(C=C_opt,class_weight='auto')\n",
    "    clf_bag_non_reduced.fit(X,Y)\n",
    "    print 'Done in {0} seconds'.format(time.time()-tic)\n",
    "\n",
    "    ###### Graph of words on feature-cleaned data #######\n",
    "    print '***** Cross-validation for linearSVM and GoW, without SVD, with feature selection *****'\n",
    "    clf=svm.LinearSVC(C=1.0,class_weight='auto')\n",
    "    tic = time.time()\n",
    "    nb_features_to_remove_at_each_step = 200 # We don't want it to be too slow\n",
    "    rfecv_graph = RFE(clf, nb_features_to_select, step=nb_features_to_remove_at_each_step)\n",
    "    rfecv_graph.fit(X_train_graph_of_words, Y)\n",
    "    toc = round(time.time() - tic,2)\n",
    "    print '{0}/{1} features selected'.format(rfecv_graph.n_features_,X_train_bag_of_words.shape[1])\n",
    "    print 'Done in {0} seconds'.format(toc)    \n",
    "    X = rfecv_graph.transform(X_train_bag_of_words)\n",
    "    X_test_graph_feature_selected = rfecv_graph.transform(X_test_graph_of_words)\n",
    "\n",
    "    tic=time.time()\n",
    "    for C_cv in C_list:\n",
    "            print 'Cost : {0}/{1}'.format(C_list.index(C_cv)+1,len(C_list))\n",
    "            clf=svm.LinearSVC(C=C_cv,class_weight='auto')\n",
    "            score_cv[(C_cv)] = np.mean(cross_validation.cross_val_score(clf, X, Y, cv=cv, scoring='f1_weighted'))\n",
    "            \n",
    "    C_opt = max(score_cv.iteritems(), key=operator.itemgetter(1))[0]\n",
    "    print 'Maximum f1 score obtained for Cost ' + str(C_opt)+' : '+ str(100*max(score_cv.values()))+ ' %'\n",
    "    \n",
    "    clf_graph_non_reduced=svm.LinearSVC(C=C_opt,class_weight='auto')\n",
    "    clf_graph_non_reduced.fit(X,Y)\n",
    "    print 'Done in {0} seconds'.format(time.time()-tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI) 4. Cross-validation score vs number of features selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation error plot for linearSVM and Bag of Words, without SVD, with feature selection\n",
      "Optimal number of features : 7675 (in 28.82 seconds)"
     ]
    }
   ],
   "source": [
    "if graph_features:\n",
    "    ## Bag of words on non-SVD-reduced data and with LinearSVM\n",
    "    X=X_train_bag_of_words\n",
    "    if use_feature_selection:\n",
    "        print 'Cross-validation error plot for linearSVM and Bag of Words, without SVD, with feature selection'\n",
    "        # The \"accuracy\" scoring is proportional to the number of correct\n",
    "        # classifications\n",
    "        from sklearn.feature_selection import RFECV\n",
    "        from sklearn.cross_validation import StratifiedKFold\n",
    "        clf=svm.LinearSVC(C=1.0,class_weight=None)\n",
    "        step=300\n",
    "        rfecv = RFECV(clf, step, cv=StratifiedKFold(Y,3, shuffle=True),scoring='f1_weighted')\n",
    "        tic = time.time()\n",
    "        rfecv.fit(X, Y)\n",
    "        toc = round(time.time() - tic,2)\n",
    "        print('Optimal number of features : {0} (in {1} seconds)'.format(rfecv.n_features_,toc))\n",
    "        \n",
    "        from matplotlib import pyplot as plt\n",
    "        # Plot number of features VS. cross-validation scores\n",
    "        plt.figure()\n",
    "        plt.xlabel(\"Number of features selected divided by step size\")\n",
    "        plt.ylabel(\"Cross validation f1 score for Bag of Words\")\n",
    "        plt.plot(range(1, len(rfecv.grid_scores_)+1), rfecv.grid_scores_)\n",
    "        plt.show()\n",
    "    \n",
    "    ## Graph of words on non-SVD-reduced data and with LinearSVM\n",
    "    X=X_train_graph_of_words\n",
    "    if use_feature_selection:\n",
    "        print 'Cross-validation error plot for linearSVM and Graph of Words, without SVD, with feature selection'\n",
    "        # The \"accuracy\" scoring is proportional to the number of correct\n",
    "        # classifications\n",
    "        clf=svm.LinearSVC(C=1.0,class_weight='auto')\n",
    "        step=300\n",
    "        rfecv = RFECV(clf, 300, cv=StratifiedKFold(Y,3),scoring='f1_weighted')\n",
    "        tic = time.time()\n",
    "        rfecv.fit(X, Y)\n",
    "        toc = round(time.time() - tic,2)\n",
    "        print('Optimal number of features : {0} (in {1} seconds)'.format(rfecv.n_features_,toc))\n",
    "    \n",
    "        # Plot number of features VS. cross-validation scores\n",
    "        plt.figure()\n",
    "        plt.xlabel(\"Number of features selected divided by step size\")\n",
    "        plt.ylabel(\"Cross validation f1 score for Graph of Words\")\n",
    "        plt.plot(range(1, len(rfecv.grid_scores_)+1), rfecv.grid_scores_)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI) 5. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost : 1/6\n",
      "Cost : 2/6\n",
      "Cost : 3/6\n",
      "Cost : 4/6\n",
      "Cost : 5/6\n",
      "Cost : 6/6\n",
      "Maximum f1 score obtained for Cost 100.0 : 97.0330960226 %\n",
      "Cost : 1/6\n",
      "Cost : 2/6\n",
      "Cost : 3/6\n",
      "Cost : 4/6\n",
      "Cost : 5/6\n",
      "Cost : 6/6\n",
      "Maximum f1 score obtained for Cost 100.0 : 97.3299228304 %\n"
     ]
    }
   ],
   "source": [
    "if string_classifier=='logistic_reg':\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    score_cv = dict()\n",
    "    C_list = [0.1,1.0,5.0,10.0,50.0,100.0]\n",
    "    \n",
    "    # Bag of Words\n",
    "    X=X_train_bag_of_words\n",
    "    for C_cv in C_list:\n",
    "            print 'Cost : {0}/{1}'.format(C_list.index(C_cv)+1,len(C_list))\n",
    "            clf=LogisticRegression(C=C_cv,class_weight='auto') # class_weight=None gives better results\n",
    "            score_cv[(C_cv)] = np.mean(cross_validation.cross_val_score(clf, X, Y, cv=cv, scoring='f1_weighted'))\n",
    "    C_opt = max(score_cv.iteritems(), key=operator.itemgetter(1))[0]\n",
    "    print 'Maximum f1 score obtained for Cost ' + str(C_opt)+' : '+ str(100*max(score_cv.values()))+ ' %'\n",
    "\n",
    "    clf_bag=LogisticRegression(C=C_opt,class_weight='auto')\n",
    "    clf_bag.fit(X,Y)\n",
    "\n",
    "    # Graph of Words\n",
    "    X=X_train_graph_of_words\n",
    "    for C_cv in C_list:\n",
    "            print 'Cost : {0}/{1}'.format(C_list.index(C_cv)+1,len(C_list))\n",
    "            clf=LogisticRegression(C=C_cv,class_weight='auto') # class_weight=None gives better results\n",
    "            score_cv[(C_cv)] = np.mean(cross_validation.cross_val_score(clf, X, Y, cv=cv, scoring='f1_weighted'))\n",
    "    C_opt = max(score_cv.iteritems(), key=operator.itemgetter(1))[0]\n",
    "    print 'Maximum f1 score obtained for Cost ' + str(C_opt)+' : '+ str(100*max(score_cv.values()))+ ' %'\n",
    "\n",
    "    clf_graph=LogisticRegression(C=C_opt,class_weight='auto')\n",
    "    clf_graph.fit(X,Y)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI) 6. Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We stay with default number of classifiers (50 Decision Trees), slow method\n",
    "if string_classifier=='Adaboost':   \n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "    clf_bag=AdaBoostClassifier()\n",
    "    clf_bag.fit(X_train_bag_of_words,Y)\n",
    "\n",
    "    clf_graph=AdaBoostClassifier()\n",
    "    clf_graph.fit(X_train_graph_of_words,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII) COMPUTE THE TEST LABELS AND SCORES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Test labels\n",
    "if with_SVD:\n",
    "    if string_classifier=='gaussianSVM':\n",
    "        Y_test_bag=clf_bag_rbf.predict(X_test_bag_svd_reduced) # Data needs to be reduced with Gaussian SVM\n",
    "        Y_test_graph=clf_graph_rbf.predict(X_test_graph_svd_reduced)\n",
    "    else:\n",
    "        Y_test_bag=clf_bag.predict(X_test_bag_svd_reduced)\n",
    "        Y_test_graph=clf_graph.predict(X_test_graph_svd_reduced)\n",
    "\n",
    "else:\n",
    "    if use_feature_selection:\n",
    "        Y_test_bag=clf_bag_non_reduced.predict(X_test_bag_feature_selected)\n",
    "        Y_test_graph=clf_graph_non_reduced.predict(X_test_graph_feature_selected)\n",
    "    else:\n",
    "        Y_test_bag=clf_bag.predict(X_test_bag_of_words)\n",
    "        Y_test_graph=clf_graph.predict(X_test_graph_of_words)\n",
    "\n",
    "## Compute the different metrics\n",
    "string_svd='No SVD'\n",
    "if with_SVD:\n",
    "    string_svd='With SVD, '+ str(n_components)+ ' components'\n",
    "string_directed='Undirected graphs'\n",
    "if isdirected:\n",
    "     string_directed='Directed graphs'\n",
    "string_weighted='Unweighted graphs'\n",
    "if isweighted:\n",
    "     string_weighted='Weighted graphs'\n",
    "\n",
    "# Print to output file\n",
    "import sys\n",
    "orig_stdout = sys.stdout\n",
    "f = file('NewResults.txt', 'a')\n",
    "sys.stdout = f\n",
    "\n",
    "print '***************************'\n",
    "print 'BAG-OF-WORDS APPROACH'\n",
    "print 'SVD : '+ str(with_SVD)\n",
    "print 'Feature Selection : ' + str(use_feature_selection)\n",
    "print string_classifier\n",
    "print\n",
    "print 'Micro-averaging : ' + str(100*sklearn.metrics.precision_score(Y_test,Y_test_bag,average='micro'))+' %'\n",
    "print 'Macro-averaging : ' + str(100*sklearn.metrics.precision_score(Y_test,Y_test_bag,average='macro'))+' %'\n",
    "print 'Weighted-averaging : ' + str(100*sklearn.metrics.precision_score(Y_test,Y_test_bag,average='weighted'))+' %'\n",
    "print '***************************'\n",
    "\n",
    "print\n",
    "print '***************************'\n",
    "print 'GRAPH-OF-WORDS APPROACH'\n",
    "print 'SVD : '+ str(with_SVD)\n",
    "print 'Feature Selection : ' + str(use_feature_selection)\n",
    "print string_classifier\n",
    "print string_directed\n",
    "print string_weighted\n",
    "print 'Window size : '+str(size_window)\n",
    "print 'Centrality measure : '+ centrality_measure\n",
    "print\n",
    "print 'Micro-averaging : ' + str(100*sklearn.metrics.precision_score(Y_test,Y_test_graph,average='micro'))+' %'\n",
    "print 'Macro-averaging : ' + str(100*sklearn.metrics.precision_score(Y_test,Y_test_graph,average='macro'))+' %'\n",
    "print 'Weighted-averaging : ' + str(100*sklearn.metrics.precision_score(Y_test,Y_test_graph,average='weighted'))+' %'\n",
    "print '***************************'\n",
    "\n",
    "sys.stdout = orig_stdout\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments :\n",
    "- As expected, linear SVMs perform best\n",
    "- Best results for weighted, undirected graphs, 3 neighbors (window size 4), 'pagerank' centrality measure, class weighting, linear SVMs\n",
    "- More computation requirements for 'pagerank', no feature selection, but $\\textbf{linear SVMs are fast}$ and do not require dimensionality reduction here\n",
    "- Gaussian SVMs are computationally unefficient, and give poor results\n",
    "- Logistic regression almost as efficient with good cross-validation and same parameters\n",
    "- Adaboost reaches $ \\approx$ 79\\% f1-score with 50 decision trees, but slow method"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
